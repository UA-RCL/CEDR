/*
 * nn_ml_sched.c
 * A pre-trained time-predictable artificial neural network, generated by Keras2C.py.
 * Based on ann.c written by keyan
 */

#include "nn_ml_sched.hpp"
#include <stdbool.h>
#include <unistd.h>

NN_ML_SCHED nn_weights_ml_sched;

// Dense init function
#include "ml_sched_l0_dense_weights"
#if ML_SCHED_L0_DENSE_USE_BIAS
#include "ml_sched_l0_dense_bias"
#endif
void init_ml_sched_l0_dense(void) {
  for (int prev_neuron_index = 0; prev_neuron_index < ML_SCHED_INPUT_DIM_0; prev_neuron_index++) {
    for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L0_DENSE_NEURON_COUNT; curr_neuron_index++) {
      nn_weights_ml_sched.ml_sched_l0_dense_weights[prev_neuron_index][curr_neuron_index] = FROM_DBL(ml_sched_l0_dense_weights[prev_neuron_index][curr_neuron_index]);
    }
  }
#if ML_SCHED_L0_DENSE_USE_BIAS
#include "ml_sched_l0_dense_bias"
  for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L0_DENSE_NEURON_COUNT; curr_neuron_index++) {
    nn_weights_ml_sched.ml_sched_l0_dense_bias[curr_neuron_index] = FROM_DBL(ml_sched_l0_dense_bias[curr_neuron_index]);
  }
#endif
}

// Dense init function
#include "ml_sched_l1_dense_weights"
#if ML_SCHED_L1_DENSE_USE_BIAS
#include "ml_sched_l1_dense_bias"
#endif
void init_ml_sched_l1_dense(void) {
  for (int prev_neuron_index = 0; prev_neuron_index < ML_SCHED_L0_DENSE_DIM_0; prev_neuron_index++) {
    for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L1_DENSE_NEURON_COUNT; curr_neuron_index++) {
      nn_weights_ml_sched.ml_sched_l1_dense_weights[prev_neuron_index][curr_neuron_index] = FROM_DBL(ml_sched_l1_dense_weights[prev_neuron_index][curr_neuron_index]);
    }
  }
#if ML_SCHED_L1_DENSE_USE_BIAS
#include "ml_sched_l1_dense_bias"
  for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L1_DENSE_NEURON_COUNT; curr_neuron_index++) {
    nn_weights_ml_sched.ml_sched_l1_dense_bias[curr_neuron_index] = FROM_DBL(ml_sched_l1_dense_bias[curr_neuron_index]);
  }
#endif
}

// Dense init function
#include "ml_sched_l2_dense_weights"
#if ML_SCHED_L2_DENSE_USE_BIAS
#include "ml_sched_l2_dense_bias"
#endif
void init_ml_sched_l2_dense(void) {
  for (int prev_neuron_index = 0; prev_neuron_index < ML_SCHED_L1_DENSE_DIM_0; prev_neuron_index++) {
    for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L2_DENSE_NEURON_COUNT; curr_neuron_index++) {
      nn_weights_ml_sched.ml_sched_l2_dense_weights[prev_neuron_index][curr_neuron_index] = FROM_DBL(ml_sched_l2_dense_weights[prev_neuron_index][curr_neuron_index]);
    }
  }
#if ML_SCHED_L2_DENSE_USE_BIAS
#include "ml_sched_l2_dense_bias"
  for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L2_DENSE_NEURON_COUNT; curr_neuron_index++) {
    nn_weights_ml_sched.ml_sched_l2_dense_bias[curr_neuron_index] = FROM_DBL(ml_sched_l2_dense_bias[curr_neuron_index]);
  }
#endif
}

// Dense init function
#include "ml_sched_l3_dense_weights"
#if ML_SCHED_L3_DENSE_USE_BIAS
#include "ml_sched_l3_dense_bias"
#endif
void init_ml_sched_l3_dense(void) {
  for (int prev_neuron_index = 0; prev_neuron_index < ML_SCHED_L2_DENSE_DIM_0; prev_neuron_index++) {
    for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L3_DENSE_NEURON_COUNT; curr_neuron_index++) {
      nn_weights_ml_sched.ml_sched_l3_dense_weights[prev_neuron_index][curr_neuron_index] = FROM_DBL(ml_sched_l3_dense_weights[prev_neuron_index][curr_neuron_index]);
    }
  }
#if ML_SCHED_L3_DENSE_USE_BIAS
#include "ml_sched_l3_dense_bias"
  for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L3_DENSE_NEURON_COUNT; curr_neuron_index++) {
    nn_weights_ml_sched.ml_sched_l3_dense_bias[curr_neuron_index] = FROM_DBL(ml_sched_l3_dense_bias[curr_neuron_index]);
    // printf("%last layer bias: lf ",nn_weights_ml_sched.ml_sched_l3_dense_bias[curr_neuron_index]);
  }
#endif
}

// create NN with pre-trained weights
void nn_init_ml_sched(void) {
  init_ml_sched_l0_dense();
  init_ml_sched_l1_dense();
  init_ml_sched_l2_dense();
  init_ml_sched_l3_dense();

  // NN_DATA_ML_SCHED nn_tmp;
  // int *feature_data = (int *)malloc(sizeof(int) * ML_SCHED_INPUT_DIM_0);
  // for (int i=0; i<ML_SCHED_INPUT_DIM_0; i++){
  // 	feature_data[i] = 10;
  // 	nn_tmp.inputs[i] = FROM_DBL(feature_data[i]);
  // }

  // nn_run_ml_sched(&nn_tmp);
  // free(feature_data);
}

// Layer run definitions
// Dense run function
// Applies activation function to a weighted sum of inputs for each neuron in this layer
void run_ml_sched_l0_dense(NN_DATA_ML_SCHED *nn_data) {
  NN_NUM_TYPE weighted_sum;
  NN_NUM_TYPE prev_neuron_value;
  NN_NUM_TYPE weight;
  NN_NUM_TYPE bias;

  // Calculate weighted sums
  for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L0_DENSE_NEURON_COUNT; curr_neuron_index++) {
    weighted_sum = 0;
    for (int prev_neuron_index = 0; prev_neuron_index < ML_SCHED_INPUT_DIM_0; prev_neuron_index++) {
      prev_neuron_value = nn_data->inputs[prev_neuron_index];
      weight = nn_weights_ml_sched.ml_sched_l0_dense_weights[prev_neuron_index][curr_neuron_index];
      weighted_sum = ADD(weighted_sum, MUL(weight, prev_neuron_value));
    }
#if ML_SCHED_L0_DENSE_USE_BIAS
    bias = nn_weights_ml_sched.ml_sched_l0_dense_bias[curr_neuron_index];
    weighted_sum = ADD(weighted_sum, bias);
#endif
    nn_data->ml_sched_l0_dense_outputs[curr_neuron_index] = weighted_sum;
  }

  // Apply activation
  NN_NUM_TYPE elem;
#if ML_SCHED_L0_DENSE_ACTIVATION == ACT_ENUM_SOFTMAX
  NN_NUM_TYPE softmax_sum = 0;

  for (int dim_0_index = 0; dim_0_index < ML_SCHED_L0_DENSE_DIM_0; dim_0_index++) {
    elem = nn_data->ml_sched_l0_dense_outputs[dim_0_index];
    elem = EXP(elem);
    softmax_sum = ADD(softmax_sum, elem);
    nn_data->ml_sched_l0_dense_outputs[dim_0_index] = elem;
  }
#endif

  for (int dim_0_index = 0; dim_0_index < ML_SCHED_L0_DENSE_DIM_0; dim_0_index++) {
    elem = nn_data->ml_sched_l0_dense_outputs[dim_0_index];

#if ML_SCHED_L0_DENSE_ACTIVATION == ACT_ENUM_SIGMOID
    elem = sigmoid(elem);
#elif ML_SCHED_L0_DENSE_ACTIVATION == ACT_ENUM_TANH
    elem = tanh(elem);
#elif ML_SCHED_L0_DENSE_ACTIVATION == ACT_ENUM_RELU
    elem = relu(elem);
#elif ML_SCHED_L0_DENSE_ACTIVATION == ACT_ENUM_LINEAR
    elem = linear(elem);
#elif ML_SCHED_L0_DENSE_ACTIVATION == ACT_ENUM_SOFTMAX
    elem = DIV(elem, softmax_sum);
#else
    printf("Invalid activation function request - %d", activation);
    exit(1);
#endif
    nn_data->ml_sched_l0_dense_outputs[dim_0_index] = elem;
  }
}

// Dense run function
// Applies activation function to a weighted sum of inputs for each neuron in this layer
void run_ml_sched_l1_dense(NN_DATA_ML_SCHED *nn_data) {
  NN_NUM_TYPE weighted_sum;
  NN_NUM_TYPE prev_neuron_value;
  NN_NUM_TYPE weight;
  NN_NUM_TYPE bias;

  // Calculate weighted sums
  for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L1_DENSE_NEURON_COUNT; curr_neuron_index++) {
    weighted_sum = 0;
    for (int prev_neuron_index = 0; prev_neuron_index < ML_SCHED_L0_DENSE_DIM_0; prev_neuron_index++) {
      prev_neuron_value = nn_data->ml_sched_l0_dense_outputs[prev_neuron_index];
      weight = nn_weights_ml_sched.ml_sched_l1_dense_weights[prev_neuron_index][curr_neuron_index];
      weighted_sum = ADD(weighted_sum, MUL(weight, prev_neuron_value));
    }
#if ML_SCHED_L1_DENSE_USE_BIAS
    bias = nn_weights_ml_sched.ml_sched_l1_dense_bias[curr_neuron_index];
    weighted_sum = ADD(weighted_sum, bias);
#endif
    nn_data->ml_sched_l1_dense_outputs[curr_neuron_index] = weighted_sum;
  }

  // Apply activation
  NN_NUM_TYPE elem;
#if ML_SCHED_L1_DENSE_ACTIVATION == ACT_ENUM_SOFTMAX
  NN_NUM_TYPE softmax_sum = 0;

  for (int dim_0_index = 0; dim_0_index < ML_SCHED_L1_DENSE_DIM_0; dim_0_index++) {
    elem = nn_data->ml_sched_l1_dense_outputs[dim_0_index];
    elem = EXP(elem);
    softmax_sum = ADD(softmax_sum, elem);
    nn_data->ml_sched_l1_dense_outputs[dim_0_index] = elem;
  }
#endif

  for (int dim_0_index = 0; dim_0_index < ML_SCHED_L1_DENSE_DIM_0; dim_0_index++) {
    elem = nn_data->ml_sched_l1_dense_outputs[dim_0_index];

#if ML_SCHED_L1_DENSE_ACTIVATION == ACT_ENUM_SIGMOID
    elem = sigmoid(elem);
#elif ML_SCHED_L1_DENSE_ACTIVATION == ACT_ENUM_TANH
    elem = tanh(elem);
#elif ML_SCHED_L1_DENSE_ACTIVATION == ACT_ENUM_RELU
    elem = relu(elem);
#elif ML_SCHED_L1_DENSE_ACTIVATION == ACT_ENUM_LINEAR
    elem = linear(elem);
#elif ML_SCHED_L1_DENSE_ACTIVATION == ACT_ENUM_SOFTMAX
    elem = DIV(elem, softmax_sum);
#else
    printf("Invalid activation function request - %d", activation);
    exit(1);
#endif
    nn_data->ml_sched_l1_dense_outputs[dim_0_index] = elem;
  }
}

// Dense run function
// Applies activation function to a weighted sum of inputs for each neuron in this layer
void run_ml_sched_l2_dense(NN_DATA_ML_SCHED *nn_data) {
  NN_NUM_TYPE weighted_sum;
  NN_NUM_TYPE prev_neuron_value;
  NN_NUM_TYPE weight;
  NN_NUM_TYPE bias;

  // Calculate weighted sums
  for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L2_DENSE_NEURON_COUNT; curr_neuron_index++) {
    weighted_sum = 0;
    for (int prev_neuron_index = 0; prev_neuron_index < ML_SCHED_L1_DENSE_DIM_0; prev_neuron_index++) {
      prev_neuron_value = nn_data->ml_sched_l1_dense_outputs[prev_neuron_index];
      weight = nn_weights_ml_sched.ml_sched_l2_dense_weights[prev_neuron_index][curr_neuron_index];
      weighted_sum = ADD(weighted_sum, MUL(weight, prev_neuron_value));
    }
#if ML_SCHED_L2_DENSE_USE_BIAS
    bias = nn_weights_ml_sched.ml_sched_l2_dense_bias[curr_neuron_index];
    weighted_sum = ADD(weighted_sum, bias);
#endif
    nn_data->ml_sched_l2_dense_outputs[curr_neuron_index] = weighted_sum;
  }

  // Apply activation
  NN_NUM_TYPE elem;
#if ML_SCHED_L2_DENSE_ACTIVATION == ACT_ENUM_SOFTMAX
  NN_NUM_TYPE softmax_sum = 0;

  for (int dim_0_index = 0; dim_0_index < ML_SCHED_L2_DENSE_DIM_0; dim_0_index++) {
    elem = nn_data->ml_sched_l2_dense_outputs[dim_0_index];
    elem = EXP(elem);
    softmax_sum = ADD(softmax_sum, elem);
    nn_data->ml_sched_l2_dense_outputs[dim_0_index] = elem;
  }
#endif

  for (int dim_0_index = 0; dim_0_index < ML_SCHED_L2_DENSE_DIM_0; dim_0_index++) {
    elem = nn_data->ml_sched_l2_dense_outputs[dim_0_index];

#if ML_SCHED_L2_DENSE_ACTIVATION == ACT_ENUM_SIGMOID
    elem = sigmoid(elem);
#elif ML_SCHED_L2_DENSE_ACTIVATION == ACT_ENUM_TANH
    elem = tanh(elem);
#elif ML_SCHED_L2_DENSE_ACTIVATION == ACT_ENUM_RELU
    elem = relu(elem);
#elif ML_SCHED_L2_DENSE_ACTIVATION == ACT_ENUM_LINEAR
    elem = linear(elem);
#elif ML_SCHED_L2_DENSE_ACTIVATION == ACT_ENUM_SOFTMAX
    elem = DIV(elem, softmax_sum);
#else
    printf("Invalid activation function request - %d", activation);
    exit(1);
#endif
    nn_data->ml_sched_l2_dense_outputs[dim_0_index] = elem;
  }
}

// Dense run function
// Applies activation function to a weighted sum of inputs for each neuron in this layer
void run_ml_sched_l3_dense(NN_DATA_ML_SCHED *nn_data) {
  NN_NUM_TYPE weighted_sum;
  NN_NUM_TYPE prev_neuron_value;
  NN_NUM_TYPE weight;
  NN_NUM_TYPE bias;
  int pl_out_index = nn_data->pl_index;

  // Calculate weighted sums
  for (int curr_neuron_index = 0; curr_neuron_index < ML_SCHED_L3_DENSE_NEURON_COUNT; curr_neuron_index++) {
    weighted_sum = 0;
    for (int prev_neuron_index = 0; prev_neuron_index < ML_SCHED_L2_DENSE_DIM_0; prev_neuron_index++) {
      prev_neuron_value = nn_data->ml_sched_l2_dense_outputs[prev_neuron_index];
      weight = nn_weights_ml_sched.ml_sched_l3_dense_weights[prev_neuron_index][curr_neuron_index];
      weighted_sum = ADD(weighted_sum, MUL(weight, prev_neuron_value));
    }
#if ML_SCHED_L3_DENSE_USE_BIAS
    bias = nn_weights_ml_sched.ml_sched_l3_dense_bias[curr_neuron_index];
    // printf("bias: %lf ",bias);
    weighted_sum = ADD(weighted_sum, bias);
    // printf("weighted_sum: %lf ",weighted_sum);
#endif
    nn_data->outputs[pl_out_index][curr_neuron_index] = weighted_sum;
  }

  // Apply activation
  NN_NUM_TYPE elem;
#if ML_SCHED_L3_DENSE_ACTIVATION == ACT_ENUM_SOFTMAX
  NN_NUM_TYPE softmax_sum = 0;

  for (int dim_0_index = 0; dim_0_index < ML_SCHED_L3_DENSE_DIM_0; dim_0_index++) {
    elem = nn_data->outputs[pl_out_index][dim_0_index];
    // printf("elem: %lf ",elem);
    elem = EXP(elem);
    // printf("exp_elem: %lf ",elem);
    softmax_sum = ADD(softmax_sum, elem);
    // printf("softmax sum: %lf ",softmax_sum);
    nn_data->outputs[pl_out_index][dim_0_index] = elem;
  }
#endif

  for (int dim_0_index = 0; dim_0_index < ML_SCHED_L3_DENSE_DIM_0; dim_0_index++) {
    elem = nn_data->outputs[pl_out_index][dim_0_index];

#if ML_SCHED_L3_DENSE_ACTIVATION == ACT_ENUM_SIGMOID
    elem = sigmoid(elem);
#elif ML_SCHED_L3_DENSE_ACTIVATION == ACT_ENUM_TANH
    elem = tanh(elem);
#elif ML_SCHED_L3_DENSE_ACTIVATION == ACT_ENUM_RELU
    elem = relu(elem);
#elif ML_SCHED_L3_DENSE_ACTIVATION == ACT_ENUM_LINEAR
    elem = linear(elem);
#elif ML_SCHED_L3_DENSE_ACTIVATION == ACT_ENUM_SOFTMAX
    elem = DIV(elem, softmax_sum);
#else
    printf("Invalid activation function request - %d", activation);
    exit(1);
#endif
    nn_data->outputs[pl_out_index][dim_0_index] = elem;
  }
}

// Must be called after every nn_run for pipelining
void pl_update_ml_sched(NN_DATA_ML_SCHED *nn_data) {
  if (++nn_data->pl_index >= ML_SCHED_MAX_PL_LEN) {
    nn_data->pl_index = 0;
  }
}

void nn_run_ml_sched(NN_DATA_ML_SCHED *nn_data) {
  run_ml_sched_l0_dense(nn_data);
  run_ml_sched_l1_dense(nn_data);
  run_ml_sched_l2_dense(nn_data);
  run_ml_sched_l3_dense(nn_data);
  pl_update_ml_sched(nn_data);
}
